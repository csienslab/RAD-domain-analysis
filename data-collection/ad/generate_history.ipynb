{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import datetime\n",
    "import braveblock\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 80% !important; }</style>\"))\n",
    "\n",
    "def parallel(func, data, process_num=None, chunksize=None, total=None, desc=None, maxtasksperchild=None):\n",
    "    process_num = mp.cpu_count() if process_num is None else process_num\n",
    "    chunksize = (total // (process_num * 32) + 1) if chunksize is None else chunksize\n",
    "    print(f'Parallel {process_num=} {chunksize=}')\n",
    "    with mp.Pool(process_num, maxtasksperchild=maxtasksperchild) as p:\n",
    "        for res in tqdm(p.imap_unordered(func, data, chunksize=chunksize), total=total, desc=desc):\n",
    "            yield res"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style>.container { width: 80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "merged_path = Path('history/merged')\n",
    "merged_path.mkdir(exist_ok=True)\n",
    "\n",
    "if not merged_path.is_dir():\n",
    "    pet = list(map(lambda x: int(x.name),[*Path('history/peterlowe').iterdir()]))\n",
    "    adg = list(map(lambda x: int(x.name),[*Path('history/adguard').iterdir()]))\n",
    "    ez = list(map(lambda x: int(x.name), [*Path('history/easylist').iterdir()]))\n",
    "    ezc = list(map(lambda x: int(x.name), [*Path('history/easylistchina').iterdir()]))\n",
    "    ezd = list(map(lambda x: int(x.name), [*Path('history/easylistdutch').iterdir()]))\n",
    "    ezf = list(map(lambda x: int(x.name), [*Path('history/easylistfr').iterdir()]))\n",
    "    ezg = list(map(lambda x: int(x.name), [*Path('history/easylistgermany').iterdir()]))\n",
    "    ezh = list(map(lambda x: int(x.name), [*Path('history/easylisthebrew').iterdir()]))\n",
    "    ezi = list(map(lambda x: int(x.name), [*Path('history/easylistitaly').iterdir()]))\n",
    "    ezp = list(map(lambda x: int(x.name), [*Path('history/easylistportuguese').iterdir()]))\n",
    "    ezs = list(map(lambda x: int(x.name), [*Path('history/easylistspanish').iterdir()]))\n",
    "    ezk = list(map(lambda x: int(x.name), [*Path('history/KoreanList').iterdir()]))\n",
    "\n",
    "    start = datetime.datetime(2015, 3, 1, 0, 0, 0)\n",
    "    end = datetime.datetime(2021, 3, 1, 0, 0, 0)\n",
    "\n",
    "    last_p, last_a, last_e = min(pet), min(adg), min(ez)\n",
    "    last_ec, last_ed, last_ef, last_eg, last_eh, last_ei, last_ep, last_es, last_ek = min(ezc), min(ezd), min(ezf), min(ezg), min(ezh), min(ezi), min(ezp), min(ezs), min(ezk), \n",
    "    for i in tqdm(range((end-start).days)):\n",
    "        day_start = (start + datetime.timedelta(days=i)).timestamp()\n",
    "        day_end = (start + datetime.timedelta(days=i+1)).timestamp()\n",
    "        p = [i for i in pet if day_start <= i < day_end]\n",
    "        a = [i for i in adg if day_start <= i < day_end if i != 1496233144]  # patch\n",
    "        e = [i for i in ez if day_start <= i < day_end]\n",
    "\n",
    "        ec = [i for i in ezc if day_start <= i < day_end]\n",
    "        ed = [i for i in ezd if day_start <= i < day_end]\n",
    "        ef = [i for i in ezf if day_start <= i < day_end]\n",
    "        eg = [i for i in ezg if day_start <= i < day_end]\n",
    "        eh = [i for i in ezh if day_start <= i < day_end]\n",
    "        ei = [i for i in ezi if day_start <= i < day_end]\n",
    "        ep = [i for i in ezp if day_start <= i < day_end]\n",
    "        es = [i for i in ezs if day_start <= i < day_end]\n",
    "        ek = [i for i in ezk if day_start <= i < day_end]\n",
    "\n",
    "        al = [*p,*a,*e,*ec,*ed,*ef,*eg,*eh,*ei,*ep,*es,*ek]\n",
    "        assert 1 <= len(al) <= 12\n",
    "        p = p if p else [last_p]\n",
    "        a = a if a else [last_a]\n",
    "        e = e if e else [last_e]\n",
    "\n",
    "        ec = ec if ec else [last_ec]\n",
    "        ed = ed if ed else [last_ed]\n",
    "        ef = ef if ef else [last_ef]\n",
    "        eg = eg if eg else [last_eg]\n",
    "        eh = eh if eh else [last_eh]\n",
    "        ei = ei if ei else [last_ei]\n",
    "        ep = ep if ep else [last_ep]\n",
    "        es = es if es else [last_es]\n",
    "        ek = ek if ek else [last_ek]\n",
    "\n",
    "        lines = []\n",
    "        lines += [f'||{l}^' for l in (Path('history/peterlowe') / str(p[0])).read_text().strip().splitlines()]\n",
    "        lines += (Path('history/adguard') / str(a[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylist') / str(e[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistchina') / str(ec[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistdutch') / str(ed[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistfr') / str(ef[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistgermany') / str(eg[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylisthebrew') / str(eh[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistitaly') / str(ei[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistportuguese') / str(ep[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/easylistspanish') / str(es[0])).read_text().strip().splitlines()\n",
    "        lines += (Path('history/KoreanList') / str(ek[0])).read_text().strip().splitlines()\n",
    "        (merged_path / str(int(day_start))).write_text('\\n'.join(lines))\n",
    "        last_p = p[0] \n",
    "        last_a = a[0]\n",
    "        last_e = e[0] \n",
    "        last_ec = ec[0] \n",
    "        last_ed = ed[0] \n",
    "        last_ef = ef[0] \n",
    "        last_eg = eg[0] \n",
    "        last_eh = eh[0] \n",
    "        last_ei = ei[0] \n",
    "        last_ep = ep[0] \n",
    "        last_es = es[0] \n",
    "        last_ek = ek[0] \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# generate merged_history.json\n",
    "import convertlists\n",
    "\n",
    "if not Path(\"../../data/merged_history.json\").exists():\n",
    "    merged_history = dict()\n",
    "    domain_list = set()\n",
    "    history_cnt = len(list(Path('history/merged').iterdir()))\n",
    "    for i, f in enumerate(tqdm(list(Path('history/merged').iterdir())[::-1])):\n",
    "        with open(f) as fd:\n",
    "            adms = convertlists.convertlist_adbp(fd, {'verbosity': 0, 'supportedoptions': {'third-party': ''}})\n",
    "        if i != history_cnt - 1:\n",
    "            for dm in adms:\n",
    "                domain_list.add(dm)\n",
    "                merged_history[dm] = datetime.datetime.utcfromtimestamp(int(f.name))\n",
    "        else:\n",
    "            for dm in adms:\n",
    "                domain_list.add(dm)\n",
    "                if dm in merged_history:\n",
    "                    del merged_history[dm]\n",
    "    with open('../../data/merged_history.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_history, f, ensure_ascii=False, indent=4, default=str)\n",
    "    with open('../../data/domain_list.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(domain_list), f, ensure_ascii=False, indent=4, default=str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "site2time_parsed = pickle.loads(Path('../../data/site2time_parsed.pickle').read_bytes())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_blocker(path):\n",
    "    return braveblock.Adblocker(rules=Path(path).read_text().strip().splitlines(), include_easylist=False, include_easyprivacy=False)\n",
    "\n",
    "def load_latest_blocker(path):\n",
    "    list_dir = Path(path)\n",
    "    path = list_dir / max([p.name for p in list_dir.iterdir()], key=int)\n",
    "    return load_blocker(str(path))\n",
    "\n",
    "latest_blockers = [\n",
    "    load_latest_blocker('history/merged'),\n",
    "]\n",
    "    \n",
    "def is_blocked(url, source_url, request_type=''):\n",
    "    return any(\n",
    "        latest_blocker.check_network_urls(url=url, source_url=source_url, request_type=request_type)\n",
    "        for latest_blocker in latest_blockers\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cache = Path('../../data/site2ad_parsed.pickle')\n",
    "\n",
    "if cache.is_file():\n",
    "    site2ad_parsed = pickle.loads(Path('../../data/site2ad_parsed.pickle').read_bytes())\n",
    "else:\n",
    "    site2ad_parsed = {site: set() for site, _ in site2time_parsed.items()}\n",
    "\n",
    "    def func(site):\n",
    "        time2parsed = site2time_parsed[site]\n",
    "        ad_parsed = set()\n",
    "        for time, parsed_urls in time2parsed.items():\n",
    "            ad_parsed.update(\n",
    "                parsed for parsed in parsed_urls if is_blocked(parsed.geturl(), f'http://{site}/')\n",
    "            )\n",
    "        return site, ad_parsed\n",
    "\n",
    "    for site, ad_parsed in parallel(func, site2time_parsed, total=len(site2time_parsed)):\n",
    "        site2ad_parsed[site].update(ad_parsed)\n",
    "\n",
    "    cache.write_bytes(pickle.dumps(site2ad_parsed))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "time_blockers = [f.name for f in list(Path('history/merged').iterdir())]\n",
    "time_blockers.sort(key=lambda t_blocker:int(t_blocker))\n",
    "\n",
    "cache_i = [1096, 1095, 548, 547, 1644, 1643, 274, 273, 822, 821, 1370, 1369, 1918, 1917, 137, 136, 411, 410, 685, 684, 959, 958, 1233, 1232, 1507, 1506, 1781, 1780, 2055, 2054, 69, 68, 206, 205, 343, 342, 480, 479, 617, 616, 754, 753, 891, 890, 1028, 1027, 1165, 1164, 1302, 1301, 1439, 1438, 1576, 1575, 1713, 1712, 1850, 1849, 1987, 1986, 2124, 2123, 35, 34, 172, 171, 309, 308, 446, 445, 583, 582, 720, 719, 857, 856, 994, 993, 1131, 1130, 1268, 1267, 1405, 1404, 1542, 1541, 1679, 1678, 1816, 1815, 1953, 1952, 2090, 2089, 103, 102, 240, 239, 377, 376, 0]\n",
    "\n",
    "time_blocker_cache = { i: load_blocker(Path('history/merged') / time_blockers[i]) for i in cache_i }\n",
    "\n",
    "def get_time_blocker(i):\n",
    "    if i in time_blocker_cache:\n",
    "        return time_blocker_cache[i]\n",
    "    return load_blocker(Path('history/merged') / time_blockers[i])\n",
    "\n",
    "def binary_search(url, site):\n",
    "    t = time_blockers[0]\n",
    "    source_url = f'http://{site}/'\n",
    "    if get_time_blocker(0).check_network_urls(url=url, source_url=source_url, request_type=''):\n",
    "        return t\n",
    "    L, R = 1, len(time_blockers)\n",
    "    while L < R:\n",
    "        M = (L + R) // 2\n",
    "        RR = get_time_blocker(M).check_network_urls(url=url, source_url=source_url, request_type='')\n",
    "        if RR:\n",
    "            LL = get_time_blocker(M - 1).check_network_urls(url=url, source_url=source_url, request_type='')\n",
    "            if LL:\n",
    "                R = M\n",
    "            else:\n",
    "                t = time_blockers[M]\n",
    "                return t\n",
    "        else:\n",
    "            L = M + 1\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from urllib.parse import urlparse\n",
    "#is_blocked(\"https://i.legendas.tv/equipe/204x25/legendas_tv_201801102138530.gif\", \"legendas.tv\")\n",
    "binary_search(\"https://i.legendas.tv/equipe/204x25/legendas_tv_201801102138530.gif\", \"legendas.tv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "binary_search(\"https://insight.adsrvr.org/track/conv/?adv=hip6dvm&ct=0:gzluor6k&fmt=3\", \"www.buffalowildwings.com\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1425168000'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import gc\n",
    "def func(s_req):\n",
    "    site, t2requests = s_req\n",
    "    \n",
    "    if (Path(\"checkpoints\") / f'{site}.pickle').is_file():\n",
    "        return site\n",
    "    \n",
    "    results = []\n",
    "    requests_set = set()\n",
    "    cache = dict()\n",
    "    \n",
    "    for requests in t2requests.values():\n",
    "        for parsed in requests:\n",
    "            requests_set.add(parsed)\n",
    "    for parsed in requests_set:\n",
    "        if is_blocked(parsed.geturl(), f'http://{site}/'):\n",
    "            t = binary_search(parsed.geturl(), site)\n",
    "            results.append((site, parsed, t))\n",
    "    (Path(\"checkpoints\") / f'{site}.pickle').write_bytes(pickle.dumps(results))\n",
    "    return site\n",
    "\n",
    "for site in parallel(func, list(site2time_parsed.items()), total=len(list(site2time_parsed)), maxtasksperchild=5, process_num=30):\n",
    "    pass"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parallel process_num=30 chunksize=1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b13b56e66b74283876b465be9cc6bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "del time_blocker_cache"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "site2_ad_parsed2t = {\n",
    "    site: {} for site in site2ad_parsed\n",
    "}\n",
    "for i in tqdm([*Path('checkpoints').iterdir()]):\n",
    "    results = pickle.loads(i.read_bytes())\n",
    "    for site, ad_parsed, t in results:\n",
    "        if t is not None:\n",
    "            site2_ad_parsed2t[site][ad_parsed] = int(t)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d6d362faf34f279cbf1b0592387d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for site in tqdm(site2ad_parsed):\n",
    "    ad_parsed = site2ad_parsed[site]\n",
    "    ad_parsed2t = site2_ad_parsed2t[site]\n",
    "    assert set(ad_parsed)  == set(ad_parsed2t.keys())"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc32fb11c39435d8fb2c5183acd25be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "Path('../../data/site2_ad_parsed2t.pickle').write_bytes(pickle.dumps(site2_ad_parsed2t))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "190362479"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = []\n",
    "def b(v):\n",
    "    global count\n",
    "    L, R = 1, 2192\n",
    "    while L < R:\n",
    "        M = (L + R) // 2\n",
    "        count.append(M)\n",
    "        count.append(M - 1)\n",
    "        RR = (v <= M)\n",
    "        if RR:\n",
    "            LL = (v <= M - 1)\n",
    "            if LL:\n",
    "                R = M\n",
    "            else:\n",
    "                return M\n",
    "        else:\n",
    "            L = M + 1\n",
    "    return None\n",
    "for i in range(1, 2192):\n",
    "    assert i == b(i)\n",
    "from collections import Counter\n",
    "print(list(dict(Counter(count).most_common(100)).keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}